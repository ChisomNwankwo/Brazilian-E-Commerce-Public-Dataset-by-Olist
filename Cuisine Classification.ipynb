{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc473e6d",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
    "from sklearn.metrics import multilabel_confusion_matrix,classification_report,f1_score,make_scorer\n",
    "\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e39c1",
   "metadata": {},
   "source": [
    "# 2. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy[['Restaurant Name', 'Cuisines']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mask = {'Restaurant Name' : 'restaurant_name', 'Cuisines' : 'cuisines'}\n",
    "\n",
    "df_copy = df_copy.rename(columns = column_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding error\n",
    "\n",
    "fix_restaurant_names = {\n",
    "    'Cafï¿½ï¿½':'CafÃ©',\n",
    "    'Pizza ï¿½ï¿½ Bessa': 'Pizza Ã  Bessa',\n",
    "    'Taypï¿½ï¿½': 'TaypÃ¡',\n",
    "    'Manzuï¿½ï¿½': 'ManzuÃ¡',\n",
    "    'Braseiro da Gï¿½ï¿½vea': 'Braseiro da GÃ¡vea',\n",
    "    'Zazï¿½ï¿½ Bistrï¿½ï¿½ Tropical': 'ZazÃ¡ BistrÃ´ Tropical',\n",
    "    'Filï¿½ï¿½ de Ouro': 'FilÃ© de Ouro',\n",
    "    'Aprazï¿½_vel': 'AprazÃ­vel',\n",
    "    'Terraï¿½_o Itï¿½ï¿½lia': 'TerraÃ§o ItÃ¡lia',\n",
    "    'Divino Fogï¿½ï¿½o': 'Divino FogÃ£o',\n",
    "    'Esquina Mocotï¿½_': 'Esquina MocotÃ³',\n",
    "    'Cevï¿½_che Tapas Bar & Restaurant': 'CevÃ­che Tapas Bar & Restaurant',\n",
    "    \"Longitude 77ï¿½ï¿½03' Bar - Le Meridien Gurgaon\": \"Longitude 77Â°03' Bar - Le Meridien Gurgaon\",\n",
    "    'buï¿½ï¿½no': 'buÃ³no',\n",
    "    'M Crï¿½ï¿½me': 'M CrÃ¨me',\n",
    "    \"Chawla'sï¿½_\": \"Chawla's\",\n",
    "    'Conï¿½_u': 'ConÃ§u',\n",
    "    'Sahibï¿½ï¿½s Barbeque by Ohriï¿½ï¿½s': \"Sahib's Barbeque by Ohri's\",\n",
    "    'NESCAFï¿½ Illusions': 'NESCAFÃ‰ Illusions',\n",
    "    'Itï¿½ï¿½s Sinful': \"Itâ€™s Sinful\",\n",
    "    'Dï¿½_ner Grill': 'DÃ¶ner Grill',\n",
    "    'LaBontï¿½ï¿½': 'LaBontÃ©',\n",
    "    'Bon Appï¿½ï¿½tit': 'Bon AppÃ©tit',\n",
    "    'Delhite Pï¿½ï¿½tisserie': 'Delhite PÃ¢tisserie',\n",
    "    'Die Bï¿½_ckerei': 'Die BÃ¤ckerei',\n",
    "    'Rosartï¿½ï¿½ Chocolate': 'RosartÃ© Chocolate',\n",
    "    'Hï¿½_agen-Dazs': 'HÃ¤agen-Dazs',\n",
    "    'TBH ï¿½ï¿½ï¿½ To Be Healthy': 'TBH â€” To Be Healthy',\n",
    "    'Caffï¿½ï¿½ La Poya': 'CaffÃ© La Poya',\n",
    "    'KBCï¿½_': 'KBC',\n",
    "    'Sautï¿½ï¿½ed Stories': 'SautÃ©ed Stories',\n",
    "    'Eden Noodles Cafe ï¿½__Î‡ï¿½_ï¿½ï¿½_ï¿½ï¿½_ï¿½ï¿½': 'Eden Noodles Cafe',\n",
    "    'Masabaï¿½ï¿½Û± Kebapï¿½_Û±sÛ±': 'MasabaÅŸ KebapÃ§Ä±s',\n",
    "    'Meï¿½ï¿½hur TavacÛ± Recep Usta': 'MeÅŸhur TavacÄ± Recep Usta',\n",
    "    'ï¿½ukuraï¿½ï¿½a SofrasÛ±': 'Ã‡ukuraÄŸa SofrasÄ±',\n",
    "    'Meï¿½ï¿½hur ï¿½_zï¿½_elik Aspava': 'MeÅŸhur Ã–zÃ§elik Aspava',\n",
    "    'Masabaï¿½ï¿½Û±': 'MasabaÅŸ',\n",
    "    'Dï¿½_veroï¿½ï¿½lu': 'DÃ¶veroÄŸlu',\n",
    "    'Pizza ï¿½ï¿½l Forno': 'Pizza Ã l Forno',\n",
    "    'Emirgan Sï¿½_tiï¿½ï¿½': 'Emirgan SÃ¼tiÅŸ',\n",
    "    'Leman Kï¿½_ltï¿½_r': 'Leman KÃ¼ltÃ¼r',\n",
    "    'Dem Karakï¿½_y': 'Dem KarakÃ¶y',\n",
    "    'Karakï¿½_y Gï¿½_llï¿½_oï¿½ï¿½lu': 'KarakÃ¶y GÃ¼llÃ¼oÄŸlu',\n",
    "    'Ceviz Aï¿½ï¿½acÛ±': 'Ceviz AÄŸacÄ±',\n",
    "    'Aï¿½ï¿½ï¿½k Kahve': 'AÃ§Ä±k Kahve'\n",
    "}\n",
    "\n",
    "df_copy['restaurant_name'] = df_copy['restaurant_name'].replace(fix_restaurant_names,regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db48d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['restaurant_name'] = df_copy['restaurant_name'][~df_copy['restaurant_name'].str.contains('ï¿½')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix encoding error in cuisines column\n",
    "\n",
    "fix_cuisines = {'Kebab, Turkish Pizza, Dï¿½_ner': 'DÃ¶ner',\n",
    "                'Desserts, Bï¿½_rek': 'Desserts, BÃ¶rek'}\n",
    "\n",
    "df_copy['cuisines'] = df_copy['cuisines'].replace(fix_cuisines,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c0a2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\s\\s+'\n",
    "df_copy['restaurant_name'] = df_copy['restaurant_name'].str.strip().replace(pattern,'',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2922dd",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multilabel classification - lets separate our labels \n",
    "cleaned_df['cuisines_list'] = cleaned_df['cuisines'].apply(lambda x:[c.strip() for c in x.split(',')])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(cleaned_df['cuisines_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's vectorized our features\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_vectorized = tfidf.fit_transform(cleaned_df['restaurant_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ecf09",
   "metadata": {},
   "source": [
    "# 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e927e",
   "metadata": {},
   "source": [
    "to avoid output like this - ` UserWarning: Label not 26 is present in all training examples. warnings.warn(`', i will filtered out labels that occur too often and less often in the df.  \n",
    "\n",
    "This message is because some label are missing from the entire training set. it could affect how well our model will learn, so lets remove those labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25744612",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = y.sum(axis=0) #how mnay times each label appears \n",
    "n_samples = y.shape[0] #total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae4824cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold: remove labels that appear in <2% or >98% of rows\n",
    "too_common = (label_count / n_samples) > 0.98\n",
    "too_rare = (label_count / n_samples) < 0.02\n",
    "\n",
    "remove_labels = np.where(too_common | too_rare)[0] #gets the exact row indices that match the filter.\n",
    "\n",
    "filtered_y = np.delete(y,remove_labels,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized,filtered_y, test_size= 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "model = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3a0812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48521702663032085\n",
      "0.40222508376329213\n",
      "0.3782003926658015\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(model,y_test,average='micro'))\n",
    "print(f1_score(model,y_test,average='macro'))\n",
    "print(f1_score(model,y_test,average='samples',zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45495ce1",
   "metadata": {},
   "source": [
    "# 5. Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea28d8",
   "metadata": {},
   "source": [
    "### 5. 1 Cross Vlidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1 = make_scorer(f1_score, average='macro')\n",
    "\n",
    "scores = cross_val_score(lr,X_vectorized, filtered_y, cv=5, scoring=f1)\n",
    "\n",
    "# Print results\n",
    "print(\"F1 Scores for each fold:\", scores)\n",
    "print(\"Average F1 Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf07aab",
   "metadata": {},
   "source": [
    "### 5.2. Usinng GridsearchCV\n",
    "\n",
    "\n",
    "let's get the parameters we will be turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'estimator__C': 5.0, 'estimator__class_weight': 'balanced', 'estimator__max_iter': 100, 'estimator__solver': 'liblinear'}\n",
      "Best F1 Score: 0.5867298617682187\n"
     ]
    }
   ],
   "source": [
    "#try gridsearchcv\n",
    "\n",
    "model = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "param_grid ={\n",
    "        'estimator__C': [1.0, 5.0],\n",
    "        'estimator__class_weight': [None, 'balanced'],\n",
    " 'estimator__max_iter': [100, 500],\n",
    " 'estimator__solver': ['liblinear'],\n",
    "}\n",
    "\n",
    "scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "#set up and run gridsearchcv\n",
    "grid_search = GridSearchCV(estimator=model,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=scorer,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1)  # use all cores\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#check result\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa2583",
   "metadata": {},
   "source": [
    "The best i could get at this point was a 58% accuracy...let me try other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2ff59",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = OneVsRestClassifier(SVC())\n",
    "\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "svc_model = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(param_grid,model):\n",
    "    scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "    #set up and run gridsearchcv\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=scorer,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1)  # use all cores\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search.best_score_,grid_search.best_params_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.556786533217986, {'estimator__C': 5.0, 'estimator__class_weight': None})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'estimator__C': [1.0,5.0],\n",
    " 'estimator__class_weight': [None,'balanced'],\n",
    "}\n",
    "\n",
    "model = svc\n",
    "svc_f1 = gridsearch(param_grid,model)\n",
    "svc_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f8c9c",
   "metadata": {},
   "source": [
    "### MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = OneVsRestClassifier(MultinomialNB())\n",
    "mn.fit(X_train, y_train)\n",
    "mn_pred = mn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49530036779730285\n",
      "0.4033025354355398\n",
      "0.39693632712500637\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test, mn_pred, average='micro'))\n",
    "print(f1_score(y_test, mn_pred, average='macro'))\n",
    "print(f1_score(y_test, mn_pred, average='samples',zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_svc = OneVsRestClassifier(LinearSVC())\n",
    "\n",
    "lr_svc.fit(X_train,y_train)\n",
    "lr_svc_pred = lr_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5910727141828654\n",
      "0.5513374983932419\n",
      "0.4949492244303565\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test, lr_svc_pred, average='micro'))\n",
    "print(f1_score(y_test, lr_svc_pred, average='macro'))\n",
    "print(f1_score(y_test, lr_svc_pred, average='samples',zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5865739623741216,\n",
       " {'estimator__C': 5.0,\n",
       "  'estimator__class_weight': 'balanced',\n",
       "  'estimator__intercept_scaling': 1,\n",
       "  'estimator__max_iter': 250})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'estimator__C': [2,5.0],\n",
    "    'estimator__max_iter': [250,500],\n",
    " 'estimator__class_weight': [None,'balanced'],\n",
    " 'estimator__intercept_scaling': [1]\n",
    "}\n",
    "\n",
    "result = gridsearch(param_grid,model)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a355989",
   "metadata": {},
   "source": [
    "# 7. Summary\n",
    "\n",
    "I started this project with a blurry understanding of NLP, and it turned out to be a great way to reacquaint myself with the core concepts.\n",
    "\n",
    "### ðŸ” What I Learned:\n",
    "1. **Handling Encoding Errors**:  \n",
    "   My dataset had several encoding issues. I initially tried fixing it by reading the file using different encoding types, but that didnâ€™t work. I eventually had to create a dictionary to manually map and correct corrupted characters. I believe the dataset was previously opened and saved using the wrong encoding, which caused those characters to lose their original meaning.\n",
    "\n",
    "2. **CountVectorizer vs TfidfVectorizer**:  \n",
    "   I learned the importance of vectorization when working with text features. I explored both CountVectorizer and TfidfVectorizer, and saw how they affect model performance and feature representation.\n",
    "\n",
    "3. **Choosing the Right Models**:  \n",
    "   With over 5,000 features, my dataset was highly dimensional. I discovered that tree-based models (like Random Forest or Decision Trees) struggle with such data because they take a long time to train. I also got introduced to the Multinomial Naive Bayes model, which is more suited for text-based classification tasks.\n",
    "\n",
    "4. **Improving Model Performance**:  \n",
    "   I experimented with hyperparameter tuning and saw how some parameters can significantly affect performance. For example, setting `class_weight='balanced'` in `LogisticRegression()` gave better results for my imbalanced labels.\n",
    "\n",
    "### âš ï¸ What Needs Improvement:\n",
    "- **Model Performance**:  \n",
    "  My current F1 score is around **60%**, which is decent for a first attempt but leaves room for improvement. I plan to revisit the data later with fresh ideas, possibly exploring different feature engineering techniques or alternative model architectures to boost performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210ba5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
