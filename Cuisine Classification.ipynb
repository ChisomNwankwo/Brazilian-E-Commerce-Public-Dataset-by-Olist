{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc473e6d",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
    "from sklearn.metrics import multilabel_confusion_matrix,classification_report,f1_score,make_scorer\n",
    "\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e39c1",
   "metadata": {},
   "source": [
    "# 2. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy[['Restaurant Name', 'Cuisines']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mask = {'Restaurant Name' : 'restaurant_name', 'Cuisines' : 'cuisines'}\n",
    "\n",
    "df_copy = df_copy.rename(columns = column_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding error\n",
    "\n",
    "fix_restaurant_names = {\n",
    "    'Caf��':'Café',\n",
    "    'Pizza �� Bessa': 'Pizza à Bessa',\n",
    "    'Tayp��': 'Taypá',\n",
    "    'Manzu��': 'Manzuá',\n",
    "    'Braseiro da G��vea': 'Braseiro da Gávea',\n",
    "    'Zaz�� Bistr�� Tropical': 'Zazá Bistrô Tropical',\n",
    "    'Fil�� de Ouro': 'Filé de Ouro',\n",
    "    'Apraz�_vel': 'Aprazível',\n",
    "    'Terra�_o It��lia': 'Terraço Itália',\n",
    "    'Divino Fog��o': 'Divino Fogão',\n",
    "    'Esquina Mocot�_': 'Esquina Mocotó',\n",
    "    'Cev�_che Tapas Bar & Restaurant': 'Cevíche Tapas Bar & Restaurant',\n",
    "    \"Longitude 77��03' Bar - Le Meridien Gurgaon\": \"Longitude 77°03' Bar - Le Meridien Gurgaon\",\n",
    "    'bu��no': 'buóno',\n",
    "    'M Cr��me': 'M Crème',\n",
    "    \"Chawla's�_\": \"Chawla's\",\n",
    "    'Con�_u': 'Conçu',\n",
    "    'Sahib��s Barbeque by Ohri��s': \"Sahib's Barbeque by Ohri's\",\n",
    "    'NESCAF� Illusions': 'NESCAFÉ Illusions',\n",
    "    'It��s Sinful': \"It’s Sinful\",\n",
    "    'D�_ner Grill': 'Döner Grill',\n",
    "    'LaBont��': 'LaBonté',\n",
    "    'Bon App��tit': 'Bon Appétit',\n",
    "    'Delhite P��tisserie': 'Delhite Pâtisserie',\n",
    "    'Die B�_ckerei': 'Die Bäckerei',\n",
    "    'Rosart�� Chocolate': 'Rosarté Chocolate',\n",
    "    'H�_agen-Dazs': 'Häagen-Dazs',\n",
    "    'TBH ��� To Be Healthy': 'TBH — To Be Healthy',\n",
    "    'Caff�� La Poya': 'Caffé La Poya',\n",
    "    'KBC�_': 'KBC',\n",
    "    'Saut��ed Stories': 'Sautéed Stories',\n",
    "    'Eden Noodles Cafe �__·�_��_��_��': 'Eden Noodles Cafe',\n",
    "    'Masaba��۱ Kebap�_۱s۱': 'Masabaş Kebapçıs',\n",
    "    'Me��hur Tavac۱ Recep Usta': 'Meşhur Tavacı Recep Usta',\n",
    "    '�ukura��a Sofras۱': 'Çukurağa Sofrası',\n",
    "    'Me��hur �_z�_elik Aspava': 'Meşhur Özçelik Aspava',\n",
    "    'Masaba��۱': 'Masabaş',\n",
    "    'D�_vero��lu': 'Döveroğlu',\n",
    "    'Pizza ��l Forno': 'Pizza àl Forno',\n",
    "    'Emirgan S�_ti��': 'Emirgan Sütiş',\n",
    "    'Leman K�_lt�_r': 'Leman Kültür',\n",
    "    'Dem Karak�_y': 'Dem Karaköy',\n",
    "    'Karak�_y G�_ll�_o��lu': 'Karaköy Güllüoğlu',\n",
    "    'Ceviz A��ac۱': 'Ceviz Ağacı',\n",
    "    'A���k Kahve': 'Açık Kahve'\n",
    "}\n",
    "\n",
    "df_copy['restaurant_name'] = df_copy['restaurant_name'].replace(fix_restaurant_names,regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db48d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['restaurant_name'] = df_copy['restaurant_name'][~df_copy['restaurant_name'].str.contains('�')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix encoding error in cuisines column\n",
    "\n",
    "fix_cuisines = {'Kebab, Turkish Pizza, D�_ner': 'Döner',\n",
    "                'Desserts, B�_rek': 'Desserts, Börek'}\n",
    "\n",
    "df_copy['cuisines'] = df_copy['cuisines'].replace(fix_cuisines,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c0a2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\s\\s+'\n",
    "df_copy['restaurant_name'] = df_copy['restaurant_name'].str.strip().replace(pattern,'',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2922dd",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multilabel classification - lets separate our labels \n",
    "cleaned_df['cuisines_list'] = cleaned_df['cuisines'].apply(lambda x:[c.strip() for c in x.split(',')])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(cleaned_df['cuisines_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's vectorized our features\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_vectorized = tfidf.fit_transform(cleaned_df['restaurant_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ecf09",
   "metadata": {},
   "source": [
    "# 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e927e",
   "metadata": {},
   "source": [
    "to avoid output like this - ` UserWarning: Label not 26 is present in all training examples. warnings.warn(`', i will filtered out labels that occur too often and less often in the df.  \n",
    "\n",
    "This message is because some label are missing from the entire training set. it could affect how well our model will learn, so lets remove those labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25744612",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = y.sum(axis=0) #how mnay times each label appears \n",
    "n_samples = y.shape[0] #total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae4824cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold: remove labels that appear in <2% or >98% of rows\n",
    "too_common = (label_count / n_samples) > 0.98\n",
    "too_rare = (label_count / n_samples) < 0.02\n",
    "\n",
    "remove_labels = np.where(too_common | too_rare)[0] #gets the exact row indices that match the filter.\n",
    "\n",
    "filtered_y = np.delete(y,remove_labels,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized,filtered_y, test_size= 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "model = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3a0812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48521702663032085\n",
      "0.40222508376329213\n",
      "0.3782003926658015\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(model,y_test,average='micro'))\n",
    "print(f1_score(model,y_test,average='macro'))\n",
    "print(f1_score(model,y_test,average='samples',zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45495ce1",
   "metadata": {},
   "source": [
    "# 5. Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea28d8",
   "metadata": {},
   "source": [
    "### 5. 1 Cross Vlidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1 = make_scorer(f1_score, average='macro')\n",
    "\n",
    "scores = cross_val_score(lr,X_vectorized, filtered_y, cv=5, scoring=f1)\n",
    "\n",
    "# Print results\n",
    "print(\"F1 Scores for each fold:\", scores)\n",
    "print(\"Average F1 Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf07aab",
   "metadata": {},
   "source": [
    "### 5.2. Usinng GridsearchCV\n",
    "\n",
    "\n",
    "let's get the parameters we will be turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'estimator__C': 5.0, 'estimator__class_weight': 'balanced', 'estimator__max_iter': 100, 'estimator__solver': 'liblinear'}\n",
      "Best F1 Score: 0.5867298617682187\n"
     ]
    }
   ],
   "source": [
    "#try gridsearchcv\n",
    "\n",
    "model = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "param_grid ={\n",
    "        'estimator__C': [1.0, 5.0],\n",
    "        'estimator__class_weight': [None, 'balanced'],\n",
    " 'estimator__max_iter': [100, 500],\n",
    " 'estimator__solver': ['liblinear'],\n",
    "}\n",
    "\n",
    "scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "#set up and run gridsearchcv\n",
    "grid_search = GridSearchCV(estimator=model,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=scorer,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1)  # use all cores\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#check result\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa2583",
   "metadata": {},
   "source": [
    "The best i could get at this point was a 58% accuracy...let me try other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2ff59",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = OneVsRestClassifier(SVC())\n",
    "\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "svc_model = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(param_grid,model):\n",
    "    scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "    #set up and run gridsearchcv\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=scorer,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1)  # use all cores\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search.best_score_,grid_search.best_params_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.556786533217986, {'estimator__C': 5.0, 'estimator__class_weight': None})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'estimator__C': [1.0,5.0],\n",
    " 'estimator__class_weight': [None,'balanced'],\n",
    "}\n",
    "\n",
    "model = svc\n",
    "svc_f1 = gridsearch(param_grid,model)\n",
    "svc_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f8c9c",
   "metadata": {},
   "source": [
    "### MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = OneVsRestClassifier(MultinomialNB())\n",
    "mn.fit(X_train, y_train)\n",
    "mn_pred = mn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49530036779730285\n",
      "0.4033025354355398\n",
      "0.39693632712500637\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test, mn_pred, average='micro'))\n",
    "print(f1_score(y_test, mn_pred, average='macro'))\n",
    "print(f1_score(y_test, mn_pred, average='samples',zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_svc = OneVsRestClassifier(LinearSVC())\n",
    "\n",
    "lr_svc.fit(X_train,y_train)\n",
    "lr_svc_pred = lr_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5910727141828654\n",
      "0.5513374983932419\n",
      "0.4949492244303565\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test, lr_svc_pred, average='micro'))\n",
    "print(f1_score(y_test, lr_svc_pred, average='macro'))\n",
    "print(f1_score(y_test, lr_svc_pred, average='samples',zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5865739623741216,\n",
       " {'estimator__C': 5.0,\n",
       "  'estimator__class_weight': 'balanced',\n",
       "  'estimator__intercept_scaling': 1,\n",
       "  'estimator__max_iter': 250})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'estimator__C': [2,5.0],\n",
    "    'estimator__max_iter': [250,500],\n",
    " 'estimator__class_weight': [None,'balanced'],\n",
    " 'estimator__intercept_scaling': [1]\n",
    "}\n",
    "\n",
    "result = gridsearch(param_grid,model)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a355989",
   "metadata": {},
   "source": [
    "# 7. Summary\n",
    "\n",
    "I started this project with a blurry understanding of NLP, and it turned out to be a great way to reacquaint myself with the core concepts.\n",
    "\n",
    "### 🔍 What I Learned:\n",
    "1. **Handling Encoding Errors**:  \n",
    "   My dataset had several encoding issues. I initially tried fixing it by reading the file using different encoding types, but that didn’t work. I eventually had to create a dictionary to manually map and correct corrupted characters. I believe the dataset was previously opened and saved using the wrong encoding, which caused those characters to lose their original meaning.\n",
    "\n",
    "2. **CountVectorizer vs TfidfVectorizer**:  \n",
    "   I learned the importance of vectorization when working with text features. I explored both CountVectorizer and TfidfVectorizer, and saw how they affect model performance and feature representation.\n",
    "\n",
    "3. **Choosing the Right Models**:  \n",
    "   With over 5,000 features, my dataset was highly dimensional. I discovered that tree-based models (like Random Forest or Decision Trees) struggle with such data because they take a long time to train. I also got introduced to the Multinomial Naive Bayes model, which is more suited for text-based classification tasks.\n",
    "\n",
    "4. **Improving Model Performance**:  \n",
    "   I experimented with hyperparameter tuning and saw how some parameters can significantly affect performance. For example, setting `class_weight='balanced'` in `LogisticRegression()` gave better results for my imbalanced labels.\n",
    "\n",
    "### ⚠️ What Needs Improvement:\n",
    "- **Model Performance**:  \n",
    "  My current F1 score is around **60%**, which is decent for a first attempt but leaves room for improvement. I plan to revisit the data later with fresh ideas, possibly exploring different feature engineering techniques or alternative model architectures to boost performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210ba5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
